# -*- coding: utf-8 -*-
"""fraud detection 63%.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15Qw0RITaBQe2ys2tXUmlSQ5HE_nnmnh0
"""

!pip install -U pandas matplotlib seaborn scikit-learn xgboost tensorflow imbalanced-learn

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from xgboost import XGBClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImPipeline

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('/content/audit_data.csv')

print("Dataset Information:")
print(df.info())
print("\nFirst few rows of the dataset:")
print(df.head())

label_encoders = {}
for column in ['Type', 'Merchant', 'Location', 'Day_of_Week', 'Account_Type']:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

print("Categorical variables encoded.")

scaler = StandardScaler()
df[['Amount', 'Time']] = scaler.fit_transform(df[['Amount', 'Time']])
print("Numerical features scaled.")

X = df.drop(columns=['Transaction_ID', 'Fraudulent'])
y = df['Fraudulent']

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

def evaluate_model(model, X_test, y_test, is_nn=False):
    if is_nn:
        # Deep learning model
        y_proba = model.predict(X_test)
        y_pred = (y_proba > 0.5).astype("int32")
    else:
        # Traditional models
        y_pred = model.predict(X_test)
        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred

    print(f"\n{model.__class__.__name__} Classification Report:")
    print(classification_report(y_test, y_pred))

    print(f"\n{model.__class__.__name__} Confusion Matrix:")
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'{model.__class__.__name__} Confusion Matrix')
    plt.show()

    auc = roc_auc_score(y_test, y_proba)
    print(f"\n{model.__class__.__name__} ROC AUC Score: {auc:.4f}")

rf = RandomForestClassifier(n_estimators=100, random_state=42)
svm = SVC(probability=True, random_state=42)
gb = GradientBoostingClassifier(n_estimators=100, random_state=42)
xgb = XGBClassifier(scale_pos_weight=len(y_train) / (2 * sum(y_train)), eval_metric='logloss', random_state=42)

for model in [rf, svm, gb, xgb]:
    model.fit(X_train, y_train)
    evaluate_model(model, X_test, y_test)

def create_nn_model(input_shape):
    model = Sequential()
    model.add(Dense(64, activation='relu', input_shape=(input_shape,)))  # INPUT SHAPE SET HERE
    model.add(Dropout(0.5))
    model.add(Dense(32, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))

print(f"Class Weights: {class_weights_dict}")

print("Class weights dictionary:")
print(class_weights_dict)

y_train = np.array(y_train)
y_test = np.array(y_test)

nn_model = create_nn_model(X_train.shape[1])

try:
    history = nn_model.fit(
        X_train, y_train,
        epochs=10,
        batch_size=32,
        validation_split=0.2,
        verbose=1,
        class_weight={0: class_weights_dict[0], 1: class_weights_dict[1]}  # UPDATED CLASS WEIGHTS FORMAT
    )
except Exception as e:
    print(f"Error during training: {e}")

plt.figure(figsize=(12, 6))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

evaluate_model(nn_model, X_test, y_test, is_nn=True)

def evaluate_model_nn(model, X_test, y_test):
  y_proba = model.predict(X_test)
  y_pred = (y_proba > 0.5).astype("int32")

  print(f"\n{model.__class__.__name__} Classification Report:")
  print(classification_report(y_test, y_pred))

  print(f"\n{model.__class__.__name__} Confusion Matrix:")
  cm = confusion_matrix(y_test, y_pred)
  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
  plt.title(f'{model.__class__.__name__} Confusion Matrix')
  plt.show()

  auc = roc_auc_score(y_test, y_proba)
  print(f"\n{model.__class__.__name__} ROC AUC Score: {auc:.4f}")

evaluate_model_nn(nn_model, X_test, y_test)